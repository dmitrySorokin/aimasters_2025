{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962eeb72-0836-4fb1-80cd-154f7d5a111f",
   "metadata": {},
   "source": [
    "# Градиент стратегии: REINFORCE.\n",
    "\n",
    "В некоторых задачах для нахождения удовлетворительной стратегии необязательно в точности знать структуру всей среды. Например, в задаче поднятия кубика робототехнической рукой вместо точной аппроксимации полезности $Q(s,a)$ достаточно знать, что выгоднее: двигаться вправо, если кубик справа, и влево в ином случае. С этим же наблюдением связано и предположение, что задача поиска удовлетворительной стратегии проще, нежели задача точной оценки функции полезности.\n",
    "\n",
    "В данном семинаре познакомимся с простейшим методом прямой оптимизации стратегии REINFORCE, в котором параметры, задающие вероятностную стратегию, изменяются в соответствии с градиентом математического ожидания отдач: \n",
    "\n",
    "$$J(\\theta)= \\mathbb{E}[G(\\tau)],$$\n",
    "\n",
    "$$\\theta \\leftarrow \\theta +\\alpha \\nabla_{\\theta} J(\\theta),$$\n",
    "\n",
    "В первом выражении математическое ожидание берется по распределениям начальных состояний, вероятностным функциям переходов и вознаграждений среды, а также по вероятностной стратегии агента $\\pi_\\theta$. Благодаря теореме о существовании стационарного распределения марковской цепи, это выражение может быть переписано через стационарное распределение $d^\\pi$ посещения состояний в среде:\n",
    "\n",
    "$$J(\\theta)= \\mathbb{E}_{s \\sim d^\\pi}[V^{\\pi}(s)] = \\mathbb{E}_{s \\sim d^\\pi, a \\sim \\pi(s)} [Q^{\\pi}(s, a)] =: \\mathbb{E}_\\pi [Q^{\\pi}(s, a)].$$\n",
    "\n",
    "Метод REINFORCE также использует теорему о градиенте стратегии, на основе которой построено целое семейство алгоритмов прямой оптимизации стратегии и актор-критиков. Теорема о градиенте стратегии связывает градиент целевой функции $J$ и градиент самой стратегии:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$$\n",
    "\n",
    "Для оценки полезности действий $Q^\\pi(s, a)$ REINFORCE использует несмещенную Монте-Карло оценку на основе полученных в среде отдач $G_t$ (за некоторое фиксированное число эпизодов в среде). С учетом этого обновление весов осуществляется по правилу:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [G_t \\nabla_\\theta \\ln \\pi_\\theta(a_t \\vert s_t)]$$\n",
    "\n",
    "Более подробное (но все равно короткое) описание градиента стратегии, и в частности REINFORCE, можно посмотреть в [блоге Lilian Weng из OpenAI](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#policy-gradient), либо полноценно разобраться в соответствующей секции классического учебника Саттона и Барто.\n",
    "\n",
    "По аналогии с DQN реализуем алгоритм REINFORCE для решения среды [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/), цель которой балансировать палочкой в вертикальном положении, управляя только тележкой, к которой она прикреплена.\n",
    "\n",
    "![cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
    "\n",
    "![cartpole](https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0762ee-f55c-4bec-b544-fb96bcdb5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cтавим нужные зависимости, если это колаб\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
    "    !pip -q install piglet\n",
    "    !pip -q install imageio_ffmpeg\n",
    "    !pip -q install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4a81a2-b188-47be-8b99-66a95401eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8fea0d-7e29-4eea-888c-26e2bf9f16c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space=Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "env.action_space=Discrete(2)\n",
      "Action_space: 2 | State_space: (4,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "print(f'{env.observation_space=}')\n",
    "print(f'{env.action_space=}')\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "print(f'Action_space: {n_actions} | State_space: {env.observation_space.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791549f-1362-401a-93da-d12588e67548",
   "metadata": {},
   "source": [
    "Вспомогательные методы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b509165-5360-44a7-9056-aed56d8feaaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_tensor(x, dtype=np.float32):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = np.asarray(x, dtype=dtype)\n",
    "    x = torch.from_numpy(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ef8b2-96c5-4a8a-bf7f-8886dc1da221",
   "metadata": {},
   "source": [
    "Сначала зададим структуру для хранения траекторий полных эпизодов для Монте-Карло оценки отдачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4733bfd8-dcce-4d83-94bd-1b95f81467de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Rollouts:\n",
    "    def __init__(self):\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.terminateds = []\n",
    "        \n",
    "    def append(self, log_prob, reward, terminated):\n",
    "        \"\"\"\n",
    "        Добавляет в хранилище логарифм вероятности выбранного действия, \n",
    "        вознаграждение и флаг терминальности состояни.\n",
    "        \"\"\"\n",
    "        self.logprobs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminateds.append(terminated)\n",
    "    \n",
    "    def get_data(self, gamma):\n",
    "        \"\"\"\n",
    "        Возвращает вектор логарифмов вероятностей совершенных \n",
    "        действий и вектор отдач.\n",
    "        \"\"\"\n",
    "        # Подготовь данные накопленных траекторий для обучения:\n",
    "        # Преобразуй вознаграждения в отдачи для этих состояний\n",
    "        # Подсказки:\n",
    "        #    1) обход списков удобнее сделать в обратном порядке, \n",
    "        #    2) не забудь сбрасывать отдачу при окончании эпизода\n",
    "        #    3) при обратном порядке построения результата \n",
    "        #       не забудь развернуть его обратно\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        # G_t = r_t + g*r_{t+1} + g^2*r_{t+2} + ..\n",
    "        rollout_len = len(self.rewards)\n",
    "        returns = np.empty(rollout_len, dtype=float)\n",
    "        ret = 0.0\n",
    "        for i in reversed(range(rollout_len)):\n",
    "            reward, terminated = self.rewards[i], self.terminateds[i]\n",
    "            returns[i] = ret = reward + gamma * ret * (not terminated)\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "        return torch.stack(self.logprobs), to_tensor(returns)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.logprobs)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.logprobs.clear()\n",
    "        self.rewards.clear()\n",
    "        self.terminateds.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045588dd-8fa1-43a0-bf7a-b5724e763fa7",
   "metadata": {},
   "source": [
    "Далее зададим отдельно класс с нейросетевой моделью агента `ActorModel` и класс самого агента, объекты которого будут содержать в себе нейросетевую модель, её оптимизатор и хранилище Монте-Карло траекторий. В конце зададим функцию обучения полученного агента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671f300d-f4fe-4396-84a7-6abe1e6ef466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=262 | avg_return=24.000\n",
      "global_step=541 | avg_return=41.500\n",
      "global_step=817 | avg_return=53.667\n",
      "global_step=1019 | avg_return=46.000\n",
      "global_step=1260 | avg_return=42.000\n",
      "global_step=1519 | avg_return=42.667\n",
      "global_step=1755 | avg_return=40.429\n",
      "global_step=2019 | avg_return=40.500\n",
      "global_step=2263 | avg_return=40.222\n",
      "global_step=2541 | avg_return=41.100\n",
      "global_step=2764 | avg_return=46.600\n",
      "global_step=3049 | avg_return=46.100\n",
      "global_step=3280 | avg_return=50.600\n",
      "global_step=3533 | avg_return=51.700\n",
      "global_step=3754 | avg_return=61.700\n",
      "global_step=4024 | avg_return=63.200\n",
      "global_step=4335 | avg_return=71.100\n",
      "global_step=4509 | avg_return=78.800\n",
      "global_step=4823 | avg_return=86.200\n",
      "global_step=5139 | avg_return=95.900\n",
      "global_step=5272 | avg_return=101.300\n",
      "global_step=5549 | avg_return=105.500\n",
      "global_step=5822 | avg_return=101.900\n",
      "global_step=6042 | avg_return=106.200\n",
      "global_step=6314 | avg_return=102.800\n",
      "global_step=6563 | avg_return=110.800\n",
      "global_step=6771 | avg_return=105.500\n",
      "global_step=7064 | avg_return=109.000\n",
      "global_step=7272 | avg_return=108.100\n",
      "global_step=7546 | avg_return=120.900\n",
      "global_step=7822 | avg_return=117.100\n",
      "global_step=8013 | avg_return=113.900\n",
      "global_step=8361 | avg_return=120.500\n",
      "global_step=8662 | avg_return=131.600\n",
      "global_step=8825 | avg_return=132.200\n",
      "global_step=9084 | avg_return=130.000\n",
      "global_step=9299 | avg_return=146.200\n",
      "global_step=9516 | avg_return=152.600\n",
      "global_step=9802 | avg_return=158.100\n",
      "global_step=10118 | avg_return=151.400\n",
      "global_step=10403 | avg_return=170.400\n",
      "global_step=10571 | avg_return=180.800\n",
      "global_step=10989 | avg_return=192.500\n",
      "global_step=11160 | avg_return=190.800\n",
      "global_step=11288 | avg_return=193.800\n",
      "global_step=11558 | avg_return=201.700\n",
      "global_step=11823 | avg_return=195.500\n",
      "global_step=12056 | avg_return=197.100\n",
      "global_step=12268 | avg_return=202.500\n",
      "global_step=12607 | avg_return=199.100\n",
      "global_step=12765 | avg_return=186.400\n",
      "global_step=13032 | avg_return=181.500\n",
      "global_step=13309 | avg_return=169.500\n",
      "global_step=13587 | avg_return=167.300\n",
      "global_step=13910 | avg_return=172.600\n",
      "global_step=14053 | avg_return=167.100\n",
      "global_step=14308 | avg_return=165.800\n",
      "global_step=14626 | avg_return=158.500\n",
      "global_step=14897 | avg_return=164.400\n",
      "global_step=15025 | avg_return=159.900\n",
      "global_step=15319 | avg_return=158.600\n",
      "global_step=15537 | avg_return=168.500\n",
      "global_step=15750 | avg_return=174.800\n",
      "global_step=16129 | avg_return=173.600\n",
      "global_step=16303 | avg_return=172.900\n",
      "global_step=16518 | avg_return=180.100\n",
      "global_step=16762 | avg_return=190.500\n",
      "global_step=17150 | avg_return=190.100\n",
      "global_step=17456 | avg_return=185.500\n",
      "global_step=17917 | avg_return=218.800\n",
      "global_step=18194 | avg_return=232.000\n",
      "global_step=18293 | avg_return=220.100\n",
      "global_step=18647 | avg_return=219.600\n",
      "global_step=18883 | avg_return=229.500\n",
      "global_step=19032 | avg_return=227.000\n",
      "global_step=19309 | avg_return=216.900\n",
      "global_step=19568 | avg_return=218.400\n",
      "global_step=19811 | avg_return=227.100\n",
      "global_step=20124 | avg_return=235.900\n",
      "global_step=20476 | avg_return=225.000\n",
      "global_step=20632 | avg_return=212.900\n",
      "global_step=20874 | avg_return=227.200\n",
      "global_step=21068 | avg_return=225.800\n",
      "global_step=21281 | avg_return=223.500\n",
      "global_step=21619 | avg_return=222.100\n",
      "global_step=21768 | avg_return=225.600\n",
      "global_step=22088 | avg_return=231.700\n",
      "global_step=22455 | avg_return=244.100\n",
      "global_step=22619 | avg_return=229.200\n",
      "global_step=22816 | avg_return=213.700\n",
      "global_step=23289 | avg_return=245.400\n",
      "global_step=23685 | avg_return=260.800\n",
      "global_step=24065 | avg_return=279.400\n",
      "global_step=24292 | avg_return=280.800\n",
      "global_step=24624 | avg_return=300.500\n",
      "global_step=24923 | avg_return=315.500\n",
      "global_step=25423 | avg_return=333.500\n",
      "global_step=25768 | avg_return=331.300\n",
      "global_step=26360 | avg_return=364.900\n",
      "global_step=26860 | avg_return=395.200\n",
      "global_step=27231 | avg_return=385.000\n",
      "global_step=27345 | avg_return=356.800\n",
      "global_step=27641 | avg_return=348.400\n",
      "global_step=28090 | avg_return=370.600\n",
      "global_step=28292 | avg_return=357.600\n",
      "global_step=28689 | avg_return=367.400\n",
      "global_step=28907 | avg_return=339.200\n",
      "global_step=29023 | avg_return=316.300\n",
      "global_step=29398 | avg_return=287.700\n",
      "global_step=29608 | avg_return=258.700\n",
      "global_step=29844 | avg_return=239.600\n",
      "global_step=30127 | avg_return=241.200\n",
      "global_step=30371 | avg_return=223.800\n",
      "global_step=30530 | avg_return=183.300\n",
      "global_step=30812 | avg_return=172.100\n",
      "global_step=31017 | avg_return=142.900\n",
      "global_step=31267 | avg_return=128.000\n",
      "global_step=31557 | avg_return=124.300\n",
      "global_step=31830 | avg_return=116.500\n",
      "global_step=32066 | avg_return=107.700\n",
      "global_step=32307 | avg_return=102.600\n",
      "global_step=32511 | avg_return=104.500\n",
      "global_step=32849 | avg_return=104.500\n",
      "global_step=33088 | avg_return=111.800\n",
      "global_step=33306 | avg_return=113.200\n",
      "global_step=33505 | avg_return=105.900\n",
      "global_step=33834 | avg_return=111.400\n",
      "global_step=34120 | avg_return=119.000\n",
      "global_step=34255 | avg_return=118.900\n",
      "global_step=34564 | avg_return=121.800\n",
      "global_step=34815 | avg_return=121.100\n",
      "global_step=35039 | avg_return=119.200\n",
      "global_step=35295 | avg_return=120.700\n",
      "global_step=35546 | avg_return=121.800\n",
      "global_step=35789 | avg_return=123.500\n",
      "global_step=36050 | avg_return=133.000\n",
      "global_step=36297 | avg_return=132.300\n",
      "global_step=36614 | avg_return=132.200\n",
      "global_step=36754 | avg_return=132.700\n",
      "global_step=37068 | avg_return=132.000\n",
      "global_step=37399 | avg_return=135.600\n",
      "global_step=37530 | avg_return=135.700\n",
      "global_step=37814 | avg_return=150.400\n",
      "global_step=38187 | avg_return=163.100\n",
      "global_step=38502 | avg_return=182.500\n",
      "global_step=38762 | avg_return=195.800\n",
      "global_step=39044 | avg_return=212.300\n",
      "global_step=39344 | avg_return=226.900\n",
      "global_step=39622 | avg_return=240.700\n",
      "global_step=39816 | avg_return=245.700\n",
      "global_step=40115 | avg_return=259.800\n",
      "global_step=40449 | avg_return=280.100\n",
      "global_step=40811 | avg_return=287.900\n",
      "global_step=41207 | avg_return=302.000\n",
      "global_step=41427 | avg_return=292.500\n",
      "global_step=41927 | avg_return=316.500\n",
      "global_step=42217 | avg_return=317.300\n",
      "global_step=42538 | avg_return=319.400\n",
      "global_step=42945 | avg_return=332.300\n",
      "global_step=43304 | avg_return=348.800\n",
      "global_step=43760 | avg_return=364.500\n",
      "global_step=44079 | avg_return=363.000\n",
      "global_step=44567 | avg_return=375.600\n",
      "global_step=44922 | avg_return=371.500\n",
      "global_step=45222 | avg_return=379.500\n",
      "global_step=45674 | avg_return=374.700\n",
      "global_step=45961 | avg_return=374.400\n",
      "global_step=46417 | avg_return=387.900\n",
      "global_step=46809 | avg_return=386.400\n",
      "global_step=47085 | avg_return=378.100\n",
      "global_step=47343 | avg_return=358.300\n",
      "global_step=47589 | avg_return=351.000\n",
      "global_step=47822 | avg_return=325.500\n",
      "global_step=48041 | avg_return=311.900\n",
      "global_step=48356 | avg_return=299.700\n",
      "global_step=48501 | avg_return=269.000\n",
      "global_step=48772 | avg_return=252.500\n",
      "global_step=49037 | avg_return=220.800\n",
      "global_step=49297 | avg_return=194.000\n",
      "global_step=49533 | avg_return=178.600\n",
      "global_step=49753 | avg_return=164.800\n",
      "global_step=50077 | avg_return=149.700\n",
      "global_step=50290 | avg_return=136.700\n",
      "global_step=50507 | avg_return=126.600\n",
      "global_step=50769 | avg_return=121.200\n",
      "global_step=51108 | avg_return=117.900\n",
      "global_step=51353 | avg_return=118.500\n",
      "global_step=51587 | avg_return=116.200\n",
      "global_step=51807 | avg_return=114.800\n",
      "global_step=52071 | avg_return=114.900\n",
      "global_step=52312 | avg_return=114.700\n",
      "global_step=52538 | avg_return=116.400\n",
      "global_step=52786 | avg_return=118.100\n",
      "global_step=53038 | avg_return=119.600\n",
      "global_step=53285 | avg_return=117.700\n",
      "global_step=53540 | avg_return=118.700\n",
      "global_step=53828 | avg_return=119.700\n",
      "global_step=54120 | avg_return=122.600\n",
      "global_step=54252 | avg_return=124.800\n",
      "global_step=54657 | avg_return=128.400\n",
      "global_step=54783 | avg_return=129.200\n",
      "global_step=55046 | avg_return=130.600\n",
      "global_step=55330 | avg_return=134.400\n",
      "global_step=55615 | avg_return=135.100\n",
      "global_step=55874 | avg_return=137.200\n",
      "global_step=56017 | avg_return=139.300\n",
      "global_step=56269 | avg_return=137.700\n",
      "global_step=56527 | avg_return=135.200\n",
      "global_step=56788 | avg_return=135.200\n",
      "global_step=57035 | avg_return=130.700\n",
      "global_step=57275 | avg_return=129.600\n",
      "global_step=57510 | avg_return=128.700\n",
      "global_step=57772 | avg_return=125.100\n",
      "global_step=58065 | avg_return=127.000\n",
      "global_step=58388 | avg_return=129.800\n",
      "global_step=58542 | avg_return=130.900\n",
      "global_step=58850 | avg_return=135.900\n",
      "global_step=59168 | avg_return=141.800\n",
      "global_step=59313 | avg_return=143.100\n",
      "global_step=59616 | avg_return=147.000\n",
      "global_step=59768 | avg_return=150.700\n",
      "global_step=60044 | avg_return=153.100\n",
      "global_step=60323 | avg_return=154.400\n",
      "global_step=60589 | avg_return=152.500\n",
      "global_step=60767 | avg_return=154.900\n",
      "global_step=61008 | avg_return=151.700\n",
      "global_step=61263 | avg_return=147.700\n",
      "global_step=61534 | avg_return=141.700\n",
      "global_step=61802 | avg_return=142.100\n",
      "global_step=62058 | avg_return=137.900\n",
      "global_step=62313 | avg_return=134.200\n",
      "global_step=62571 | avg_return=131.200\n",
      "global_step=62823 | avg_return=133.000\n",
      "global_step=63083 | avg_return=130.300\n",
      "global_step=63374 | avg_return=127.600\n",
      "global_step=63582 | avg_return=136.200\n",
      "global_step=63853 | avg_return=136.600\n",
      "global_step=64014 | avg_return=140.800\n",
      "global_step=64302 | avg_return=140.800\n",
      "global_step=64663 | avg_return=150.300\n",
      "global_step=64818 | avg_return=154.300\n",
      "global_step=65140 | avg_return=160.100\n",
      "global_step=65322 | avg_return=163.000\n",
      "global_step=65539 | avg_return=173.400\n",
      "global_step=65917 | avg_return=176.200\n",
      "global_step=66114 | avg_return=175.100\n",
      "global_step=66308 | avg_return=180.900\n",
      "global_step=66686 | avg_return=202.600\n",
      "global_step=66898 | avg_return=208.900\n",
      "global_step=67073 | avg_return=205.800\n",
      "global_step=67534 | avg_return=219.400\n",
      "global_step=68003 | avg_return=232.000\n",
      "global_step=68343 | avg_return=229.100\n",
      "global_step=68513 | avg_return=224.400\n",
      "global_step=68858 | avg_return=223.300\n",
      "global_step=69044 | avg_return=222.200\n",
      "global_step=69353 | avg_return=216.800\n",
      "global_step=69513 | avg_return=195.000\n",
      "global_step=69783 | avg_return=185.800\n",
      "global_step=70034 | avg_return=178.300\n",
      "global_step=70271 | avg_return=160.900\n",
      "global_step=70514 | avg_return=142.800\n",
      "global_step=70876 | avg_return=141.700\n",
      "global_step=71003 | avg_return=137.400\n",
      "global_step=71261 | avg_return=133.500\n",
      "global_step=71511 | avg_return=127.600\n",
      "global_step=71765 | avg_return=126.900\n",
      "global_step=72024 | avg_return=123.300\n",
      "global_step=72294 | avg_return=125.100\n",
      "global_step=72561 | avg_return=127.800\n",
      "global_step=72863 | avg_return=130.800\n",
      "global_step=73026 | avg_return=135.700\n",
      "global_step=73362 | avg_return=138.600\n",
      "global_step=73519 | avg_return=141.600\n",
      "global_step=73826 | avg_return=144.400\n",
      "global_step=74173 | avg_return=150.600\n",
      "global_step=74331 | avg_return=153.100\n",
      "global_step=74648 | avg_return=155.700\n",
      "global_step=74814 | avg_return=158.500\n",
      "global_step=75120 | avg_return=160.500\n",
      "global_step=75262 | avg_return=160.000\n",
      "global_step=75612 | avg_return=162.200\n",
      "global_step=75760 | avg_return=159.900\n",
      "global_step=76071 | avg_return=160.800\n",
      "global_step=76382 | avg_return=159.500\n",
      "global_step=76535 | avg_return=155.900\n",
      "global_step=76848 | avg_return=155.500\n",
      "global_step=77014 | avg_return=157.100\n",
      "global_step=77354 | avg_return=156.000\n",
      "global_step=77539 | avg_return=159.800\n",
      "global_step=77865 | avg_return=161.200\n",
      "global_step=78018 | avg_return=158.000\n",
      "global_step=78300 | avg_return=156.100\n",
      "global_step=78610 | avg_return=153.600\n",
      "global_step=78759 | avg_return=154.100\n",
      "global_step=79056 | avg_return=153.600\n",
      "global_step=79347 | avg_return=153.200\n",
      "global_step=79632 | avg_return=149.900\n",
      "global_step=79779 | avg_return=149.100\n",
      "global_step=80042 | avg_return=143.400\n",
      "global_step=80333 | avg_return=142.900\n",
      "global_step=80600 | avg_return=139.600\n",
      "global_step=80876 | avg_return=140.900\n",
      "global_step=81011 | avg_return=140.300\n",
      "global_step=81291 | avg_return=140.200\n",
      "global_step=81567 | avg_return=139.300\n",
      "global_step=81857 | avg_return=139.200\n",
      "global_step=82009 | avg_return=141.100\n",
      "global_step=82270 | avg_return=139.500\n",
      "global_step=82536 | avg_return=140.200\n",
      "global_step=82834 | avg_return=140.900\n",
      "global_step=83128 | avg_return=144.300\n",
      "global_step=83277 | avg_return=145.000\n",
      "global_step=83593 | avg_return=147.300\n",
      "global_step=83769 | avg_return=150.100\n",
      "global_step=84121 | avg_return=152.900\n",
      "global_step=84304 | avg_return=156.300\n",
      "global_step=84668 | avg_return=160.400\n",
      "global_step=84880 | avg_return=168.500\n",
      "global_step=85082 | avg_return=175.200\n",
      "global_step=85310 | avg_return=182.200\n",
      "global_step=85725 | avg_return=189.900\n",
      "global_step=85935 | avg_return=196.000\n",
      "global_step=86173 | avg_return=204.000\n",
      "global_step=86387 | avg_return=207.800\n",
      "global_step=86678 | avg_return=220.200\n",
      "global_step=86955 | avg_return=229.600\n",
      "global_step=87273 | avg_return=242.100\n",
      "global_step=87572 | avg_return=250.800\n",
      "global_step=87942 | avg_return=267.600\n",
      "global_step=88342 | avg_return=284.800\n",
      "global_step=88723 | avg_return=299.800\n",
      "global_step=89223 | avg_return=328.800\n",
      "global_step=89723 | avg_return=355.000\n",
      "global_step=90223 | avg_return=383.600\n",
      "global_step=90723 | avg_return=404.500\n",
      "global_step=91223 | avg_return=426.800\n",
      "global_step=91723 | avg_return=445.000\n",
      "global_step=92223 | avg_return=465.100\n",
      "global_step=92723 | avg_return=478.100\n",
      "global_step=93223 | avg_return=488.100\n",
      "global_step=93723 | avg_return=500.000\n",
      "Решено!\n"
     ]
    }
   ],
   "source": [
    "class ActorModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Инициализируйте сеть агента с одной головой \n",
    "        # softmax-актором: `net` и `actor_head`\n",
    "        # NB: сразу разделим на тело и голову, тк в будущем на базе \n",
    "        # этой архитектуры будем делать уже актор-критика \n",
    "        # c телом и двумя головами\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        from_dim = input_dim\n",
    "        layers = []\n",
    "        for to_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(from_dim, to_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            from_dim = to_dim\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.actor_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Вычислите выбранное действие и логарифм вероятности его выбора:\n",
    "        # Для этого вам пригодится `Categorical` из torch.distributions — \n",
    "        #   у него есть методы sample и log_prob.\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        state = self.net(state)\n",
    "        # state = [0.1, -0.2, 0.05, 0.1]\n",
    "        # -> 4 x 128\n",
    "        # state = [-0.1, 0.3, ..., 0.2]  128x\n",
    "\n",
    "        action_probs = self.actor_head(state)\n",
    "        # 128 -> 2\n",
    "        # action_probs = [0.7, 0.3]\n",
    "\n",
    "        dist = Categorical(probs=action_probs)\n",
    "        # # dist теперь представляет распределение с P(action=0) = 0.7, P(action=1) = 0.3\n",
    "\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        action = action.item()\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class ReinforceAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Инициализируйте модель агента и ее SGD оптимизатор\n",
    "        #   (например, `torch.optim.Adam`)\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        self.actor = ActorModel(state_dim, hidden_dims, action_dim)\n",
    "        self.opt = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "\n",
    "        self.rollouts = Rollouts()\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Произведите выбор действия\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        state = to_tensor(state)\n",
    "        action, logprob = self.actor(state) \n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        return action, logprob\n",
    "        \n",
    "    def append_to_rollouts(self, logprob, r, done):\n",
    "        # Добавьте новый экземпляр данных в память роллаутов.\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        self.rollouts.append(logprob, r, done)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "    def update(self, min_data_size=1):\n",
    "        if len(self.rollouts) < min_data_size:\n",
    "            # Пропускаем шаг обновления, если собрали мало данных\n",
    "            return\n",
    "        \n",
    "        self.update_actor()\n",
    "\n",
    "        # Очищаем накопленные роллауты после шага обучения\n",
    "        self.rollouts.clear()\n",
    "\n",
    "    def update_actor(self):\n",
    "        logprobs, returns = self.rollouts.get_data(self.gamma)\n",
    "\n",
    "        # Реализуйте шаг обновления актора: вычислите ошибку `loss` \n",
    "        # и произведите шаг обновления градиентным спуском. \n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        loss = -torch.mean(returns * logprobs)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        \"\"\"</codehere>\"\"\"\n",
    "\n",
    "def run_reinforce_actor(\n",
    "    env_name=\"CartPole-v1\", \n",
    "    hidden_dims=(128, 128), \n",
    "    lr=1e-3, \n",
    "    gamma=0.99,\n",
    "    total_max_steps=200_000,\n",
    "    # данные скольких эпизодов используются для одного шага обновления\n",
    "    train_schedule_episodes=1, \n",
    "    min_data_size=64,\n",
    "    eval_schedule=1000, \n",
    "    smooth_ret_window=10, \n",
    "    success_ret=500.\n",
    "):\n",
    "    env = gym.make(env_name)\n",
    "    episode_return_history = deque(maxlen=smooth_ret_window)\n",
    "\n",
    "    agent = ReinforceAgent(\n",
    "        state_dim=env.observation_space.shape[0], \n",
    "        action_dim=env.action_space.n, \n",
    "        hidden_dims=hidden_dims, \n",
    "        lr=lr, \n",
    "        gamma=gamma\n",
    "    )\n",
    "    \n",
    "    s, _ = env.reset()\n",
    "    done, episode_return = False, 0.\n",
    "    i_episode = 0\n",
    "    eval = False\n",
    "\n",
    "    for global_step in range(1, total_max_steps+1):\n",
    "\n",
    "        a, logprob = agent.act(s)\n",
    "\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        episode_return += r\n",
    "        \n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.append_to_rollouts(logprob, r, terminated)\n",
    "\n",
    "        # evaluate\n",
    "        if global_step % eval_schedule == 0:\n",
    "            eval = True\n",
    "\n",
    "        s = s_next\n",
    "        if done:\n",
    "            i_episode += 1\n",
    "            # обучение по расписанию (на число эпизодов)\n",
    "            if i_episode % train_schedule_episodes == 0:\n",
    "                agent.update(min_data_size)\n",
    "\n",
    "            if eval:\n",
    "                # для этого алгоритма не будем делать шаг оценки \n",
    "                # (но можно было бы, например, использовать жадную стратегию),\n",
    "                # поэтому вместо этого сохраним результаты последнего \n",
    "                # тренировочного эпизода\n",
    "                episode_return_history.append(episode_return)\n",
    "                avg_return = np.mean(episode_return_history)\n",
    "                print(f'{global_step=} | {avg_return=:.3f}')\n",
    "                if avg_return >= success_ret:\n",
    "                    print('Решено!')\n",
    "                    break\n",
    "\n",
    "            s, _ = env.reset()\n",
    "            done, episode_return = False, 0.\n",
    "            eval = False\n",
    "\n",
    "run_reinforce_actor(eval_schedule=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c91fe2",
   "metadata": {},
   "source": [
    "# Actor-Critic\n",
    "\n",
    "Теорема о градиенте стратегии связывает градиент целевой функции  и градиент самой стратегии:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$$\n",
    "\n",
    "Встает вопрос, как оценить $Q^\\pi(s, a)$? В чистом policy-based алгоритме REINFORCE используется отдача $G_t$, полученная методом Монте-Карло в качестве несмещенной оценки $Q^\\pi(s, a)$. В Actor-Critic же предлагается отдельно обучать нейронную сеть Q-функции — критика.\n",
    "\n",
    "Актор-критиком часто называют обобщенный фреймворк (подход), нежели какой-то конкретный алгоритм. Как подход актор-критик не указывает, каким конкретно [policy gradient] методом обучается актор и каким [value based] методом обучается критик. Таким образом актор-критик задает целое [семейство](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) различных алгоритмов. Рекомендую в качестве шпаргалки использовать упомянутый в тетрадке с REINFORCE [пост из блога Lilian Weng](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/), посвященный наиболее популярным алгоритмам семейства актор-критиков\n",
    "\n",
    "В данной тетрадке познакомимся с наиболее простым вариантом актор-критика, который так и называют Actor-Critic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30d839",
   "metadata": {},
   "source": [
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla\\theta \\ln \\pi_\\theta(a|s)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bfbbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example:\n",
    "# # Вариант с общим телом\n",
    "# self.net = nn.Sequential(*create_body())  # Общие слои\n",
    "# self.actor_head = nn.Sequential(...)      # Специфичные слои для актора\n",
    "# self.critic_head = nn.Sequential(...)     # Специфичные слои для критика\n",
    "\n",
    "# # При прямом проходе:\n",
    "# state = self.net(state)           # Сначала через общее тело\n",
    "# action_probs = self.actor_head(state)  # Затем через голову актора\n",
    "# q_values = self.critic_head(state)     # Или через голову критика\n",
    "\n",
    "# # [+] Меньше параметров (экономия памяти)\n",
    "# # [+] Может быть полезно, если признаки, извлекаемые из состояния, полезны для обеих задач\n",
    "# # [-] Может возникать конфликт градиентов при обучении\n",
    "# # [-] Менее гибкая архитектура\n",
    "\n",
    "\n",
    "# # Вариант без общего тела\n",
    "# self.net = None\n",
    "# self.actor_head = nn.Sequential(\n",
    "#     *create_body(),  # Полная сеть актора\n",
    "#     nn.Linear(hidden_dims[-1], output_dim),\n",
    "#     nn.Softmax(dim=-1),\n",
    "# )\n",
    "# self.critic_head = nn.Sequential(\n",
    "#     *create_body(),  # Полная сеть критика\n",
    "#     nn.Linear(hidden_dims[-1], output_dim),\n",
    "# )\n",
    "\n",
    "# # [+] Нет конфликта градиентов\n",
    "# # [+] Каждая сеть может специализироваться на своей задаче\n",
    "# # [+] Более стабильное обучение\n",
    "# # [-] Больше параметров\n",
    "# # [-] Требует больше вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a231ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, dtype=np.float32):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = np.asarray(x, dtype=dtype)\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "def symlog(x):\n",
    "    \"\"\"Compute symlog values for a vector `x`. It's an inverse operation for symexp.\"\"\"\n",
    "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "def symexp(x):\n",
    "    \"\"\"Compute symexp values for a vector `x`. It's an inverse operation for symlog.\"\"\"\n",
    "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
    "\n",
    "\n",
    "class SymExpModule(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return symexp(x)\n",
    "\n",
    "\n",
    "# ======== Код ниже взят из тетрадки по DQN =======    \n",
    "def select_action_eps_greedy(Q, state, epsilon):\n",
    "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "    Q_s = Q(state).detach().numpy()\n",
    "    \n",
    "    # action = \n",
    "    \"\"\"<codehere>\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        n_actions = Q_s.shape[-1]\n",
    "        action = np.random.choice(n_actions)\n",
    "    else:\n",
    "        action = np.argmax(Q_s)\n",
    "    \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "    action = int(action)\n",
    "\n",
    "    return action\n",
    "\n",
    "def get_batch(replay_buffer):\n",
    "    # split an array of samples into arrays: states, actions, rewards, next_actions, terminateds\n",
    "    \"\"\"<codehere>\"\"\"\n",
    "    states, actions, rewards, next_states, terminateds = [], [], [], [], []\n",
    "    for s, a, r, n_s, done in replay_buffer:\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        next_states.append(n_s)\n",
    "        terminateds.append(done)\n",
    "\n",
    "    \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminateds)\n",
    "    # return map(np.array, [states, actions, rewards, next_states, terminateds])\n",
    "\n",
    "def sample_batch(replay_buffer, n_samples):\n",
    "    # sample randomly `n_samples` samples from replay buffer\n",
    "    # and split an array of samples into arrays: states, actions, rewards, next_states, terminateds\n",
    "    \"\"\"<codehere>\"\"\"\n",
    "    n_samples = min(len(replay_buffer), n_samples)\n",
    "\n",
    "    indices = np.random.choice(len(replay_buffer), n_samples, replace=False)\n",
    "\n",
    "    states, actions, rewards, next_states, terminateds = [], [], [], [], []\n",
    "    for i in indices:\n",
    "        s, a, r, n_s, done = replay_buffer[i]\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        next_states.append(n_s)\n",
    "        terminateds.append(done)\n",
    "\n",
    "    \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminateds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a100ddf",
   "metadata": {},
   "source": [
    "## Shared-body Actor-Critic\n",
    "\n",
    "Актор и критик могут обучаться в разных режимах — актор только on-policy (шаг обучения на текущей собранной подтраектории), а критик on-policy или off-policy (шаг обучения на текущей подтраектории или на батче из replay buffer). Это с одной стороны привносит гибкость в обучение, с другой — усложняет его.\n",
    "\n",
    "Если актор и критик оба обучаются on-policy, то имеет смысл объединить их сетки в одну и делать общий шаг обратного распространения ошибки. Однако, если они обучаются в разных режимах (и с разной частотой обновления), то велика вероятность, что их шаги обучения могут начать конфликтовать в случае общего тела — для такого варианта намного предпочтительнее разделить их на разные подсети (либо аккуратно настраивать гиперпарметры, чтобы стабилизировать обучение). В целом, рекомендуется использовать общий энкодер наблюдений, а далее как можно скорее разделять головы.\n",
    "\n",
    "Далее сначала попробуем реализацию актор-критика с общим телом и с on-policy вариантом обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8143ec",
   "metadata": {},
   "source": [
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla\\theta \\ln \\pi_\\theta(a|s)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8af0642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=2111 | avg_return=135.000\n",
      "global_step=4028 | avg_return=116.000\n",
      "global_step=6093 | avg_return=120.333\n",
      "global_step=8020 | avg_return=189.750\n",
      "global_step=10041 | avg_return=199.400\n",
      "global_step=12080 | avg_return=223.833\n",
      "global_step=14067 | avg_return=203.571\n",
      "global_step=16044 | avg_return=196.875\n",
      "global_step=18079 | avg_return=189.111\n",
      "global_step=20101 | avg_return=186.300\n",
      "global_step=22124 | avg_return=190.400\n",
      "global_step=24179 | avg_return=199.400\n",
      "global_step=26020 | avg_return=214.000\n",
      "global_step=28169 | avg_return=194.600\n",
      "global_step=30062 | avg_return=192.800\n",
      "global_step=32013 | avg_return=183.500\n",
      "global_step=34068 | avg_return=186.500\n",
      "global_step=36022 | avg_return=189.200\n",
      "global_step=38179 | avg_return=199.100\n",
      "global_step=40085 | avg_return=207.500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 263\u001b[0m\n\u001b[1;32m    260\u001b[0m             done, episode_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m \u001b[43mrun_actor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_max_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 243\u001b[0m, in \u001b[0;36mrun_actor_critic\u001b[0;34m(env_name, hidden_dims, lr, total_max_steps, train_schedule, replay_buffer_size, eval_schedule, smooth_ret_window, success_ret)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# train step\u001b[39;00m\n\u001b[1;32m    242\u001b[0m agent\u001b[38;5;241m.\u001b[39mappend_to_replay_buffer(s, a, r, s_next, terminated)\n\u001b[0;32m--> 243\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_schedule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m eval_schedule \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[11], line 140\u001b[0m, in \u001b[0;36mActorCriticAgent.update\u001b[0;34m(self, rollout_size)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 140\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_actor()\n\u001b[1;32m    142\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[11], line 166\u001b[0m, in \u001b[0;36mActorCriticAgent.update_critic\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_rb)\n\u001b[1;32m    164\u001b[0m states, actions, rewards, next_states, terminateds \u001b[38;5;241m=\u001b[39m train_batch\n\u001b[0;32m--> 166\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_td_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminateds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"</codehere>\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[11], line 196\u001b[0m, in \u001b[0;36mActorCriticAgent.compute_td_loss\u001b[0;34m(self, states, actions, rewards, next_states, terminateds, regularizer)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"<codehere>\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 196\u001b[0m     Q_sn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_next\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     V_sn, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(Q_sn, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"</codehere>\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 133\u001b[0m, in \u001b[0;36mActorCriticAgent.evaluate\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 89\u001b[0m, in \u001b[0;36mActorCriticModel.evaluate\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"<codehere>\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(state)\n\u001b[0;32m---> 89\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"</codehere>\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ppo/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ppo/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ppo/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ppo/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ppo/lib/python3.9/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1736\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m-> 1740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1741\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ActorBatch:\n",
    "    def __init__(self):\n",
    "        self.logprobs = []\n",
    "        self.q_values = []\n",
    "        \n",
    "    def append(self, log_prob, q_value):\n",
    "        self.logprobs.append(log_prob)\n",
    "        self.q_values.append(q_value)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.logprobs.clear()\n",
    "        self.q_values.clear()\n",
    "\n",
    "    \n",
    "class ActorCriticModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Инициализируйте сеть агента с двумя головами: softmax-актора и линейного критика\n",
    "        # self.net, self.actor_head, self.critic_head =\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        def create_body():\n",
    "            from_dim = input_dim\n",
    "            layers = []\n",
    "            for to_dim in hidden_dims:\n",
    "                layers.extend([\n",
    "                    nn.Linear(from_dim, to_dim),\n",
    "                    nn.Tanh()\n",
    "                ])\n",
    "                from_dim = to_dim\n",
    "            return layers\n",
    "        \n",
    "        self.net = nn.Sequential(*create_body())\n",
    "\n",
    "        self.actor_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.critic_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], output_dim),\n",
    "\n",
    "        )\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Вычислите выбранное действие, логарифм вероятности его выбора и соответствующее значение Q-функции\n",
    "        # Опционально, здесь можно вместо Q[s, a] возвращать A[s, a] — тогда это будет аналог Advantage Actor-Critic (A2C)\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "\n",
    "        state = self.net(state)\n",
    "\n",
    "        action_probs = self.actor_head(state)\n",
    "\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        action = action.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q_s = self.critic_head(state)\n",
    "            # Например: Q_s = [1.5, 0.8]\n",
    "\n",
    "            Q_s_a = Q_s[action]\n",
    "            # action=0: Q_s_a = 1.5\n",
    "\n",
    "            V_s, _ = torch.max(Q_s, axis=-1)\n",
    "            # V_s = 1.5 (максимум из [1.5, 0.8])\n",
    "\n",
    "            advantage = Q_s_a - V_s\n",
    "            # advantage = 1.5 - 1.5 = 0 (если выбрали лучшее действие)\n",
    "            # advantage = 0.8 - 1.5 = -0.7 (если выбрали худшее действие)\n",
    "\n",
    "            Q_s_a = advantage\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "        return action, log_prob, Q_s_a\n",
    "    \n",
    "    def evaluate(self, state):\n",
    "        # Вычислите значения Q-функции для данного состояния\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        state = self.net(state)\n",
    "\n",
    "        q_values = self.critic_head(state)\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims, lr, gamma, critic_rb_size):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Инициализируйте модель актор-критика и SGD оптимизатор (например, `torch.optim.Adam)`)\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        self.actor_critic = ActorCriticModel(state_dim, hidden_dims, action_dim)\n",
    "        self.opt = torch.optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "\n",
    "        self.actor_batch = ActorBatch()\n",
    "        self.critic_rb = deque(maxlen=critic_rb_size)\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Произведите выбор действия и сохраните необходимые данные в батч для последующего обучения\n",
    "        # Не забудьте сделать q_value.detach()\n",
    "        # self.actor_batch.append(..)\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        state = to_tensor(state)\n",
    "        action, logprob, q_value = self.actor_critic(state)\n",
    "\n",
    "        self.actor_batch.append(logprob, q_value.detach())\n",
    "\n",
    "\n",
    "        # NB: Для дебага, можно сначала обучать только критика и убедиться, что DQN работает\n",
    "        # action = select_action_eps_greedy(self.actor_critic.critic_head, state, 0.05)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def append_to_replay_buffer(self, s, a, r, next_s, terminated):\n",
    "        # Добавьте новый экземпляр данных в память прецедентов.\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        self.critic_rb.append((s, a, r, next_s, terminated))\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "    def evaluate(self, state):\n",
    "        return self.actor_critic.evaluate(state)\n",
    "    \n",
    "    def update(self, rollout_size):\n",
    "        if len(self.actor_batch.q_values) < rollout_size:\n",
    "            return\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss = self.update_critic()\n",
    "        loss += self.update_actor()\n",
    "        loss.backward()\n",
    "\n",
    "        self.opt.step()\n",
    "        self.actor_batch.clear()\n",
    "        self.critic_rb.clear()\n",
    "\n",
    "    def update_actor(self):\n",
    "        Q_s_a = to_tensor(self.actor_batch.q_values)\n",
    "        logprobs = torch.stack(self.actor_batch.logprobs)\n",
    "\n",
    "        # Реализуйте шаг обновления актора — вычислите ошибку `loss` и произведите шаг обновления градиентным спуском. \n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        # Считаем ошибку\n",
    "        loss = -torch.mean(Q_s_a * logprobs)\n",
    "        \n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        return loss\n",
    "    \n",
    "    def update_critic(self):\n",
    "        # Реализуйте n_updates шагов обучения критика.\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        train_batch = get_batch(self.critic_rb)\n",
    "        states, actions, rewards, next_states, terminateds = train_batch\n",
    "\n",
    "        loss = self.compute_td_loss(states, actions, rewards, next_states, terminateds)\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        return loss\n",
    "        \n",
    "    def compute_td_loss(\n",
    "        self, states, actions, rewards, next_states, terminateds, regularizer=0.1\n",
    "    ):\n",
    "        # переводим входные данные в тензоры\n",
    "        s = to_tensor(states)                     # shape: [batch_size, state_size]\n",
    "        a = to_tensor(actions, int).long()        # shape: [batch_size]\n",
    "        r = to_tensor(rewards)                    # shape: [batch_size]\n",
    "        s_next = to_tensor(next_states)           # shape: [batch_size, state_size]\n",
    "        term = to_tensor(terminateds, bool)       # shape: [batch_size]\n",
    "\n",
    "        \n",
    "        # получаем Q[s, a] для выбранных действий в текущих состояниях (для каждого примера из батча)\n",
    "        # Q_s_a = ...\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        Q_s_a = torch.gather(\n",
    "            self.evaluate(s), dim=1, index=torch.unsqueeze(a, 1)\n",
    "        ).squeeze(1)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "        # получаем Q[s_next, *] — значения полезности всех действий в следующих состояниях\n",
    "        # Q_sn = ...,\n",
    "        # а затем вычисляем V*[s_next] — оптимальные значения полезности следующих состояний\n",
    "        # V_sn = ...\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        with torch.no_grad():\n",
    "            Q_sn = self.evaluate(s_next)\n",
    "            V_sn, _ = torch.max(Q_sn, axis=-1)\n",
    "\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "        # вычисляем TD target и далее TD error\n",
    "        # target = ...\n",
    "        # td_error = ...\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        target = r + self.gamma * V_sn * torch.logical_not(term)\n",
    "        td_error = target - Q_s_a\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "        # MSE loss для минимизации\n",
    "        loss = torch.mean(td_error ** 2)\n",
    "        # добавляем регуляризацию на значения Q \n",
    "        loss += regularizer * Q_s_a.mean()\n",
    "        return loss\n",
    "\n",
    "def run_actor_critic(\n",
    "        env_name=\"CartPole-v1\", \n",
    "        hidden_dims=(128, 128), lr=1e-3,\n",
    "        total_max_steps=200_000,\n",
    "        train_schedule=1, replay_buffer_size=5000,\n",
    "        eval_schedule=1000, smooth_ret_window=10, success_ret=500.\n",
    "):\n",
    "    env = gym.make(env_name)\n",
    "    episode_return_history = deque(maxlen=smooth_ret_window)\n",
    "\n",
    "    agent = ActorCriticAgent(\n",
    "        state_dim=env.observation_space.shape[0], \n",
    "        action_dim=env.action_space.n, hidden_dims=hidden_dims,\n",
    "        lr=lr, gamma=.995, critic_rb_size=replay_buffer_size\n",
    "    )\n",
    "    \n",
    "    s, _ = env.reset()\n",
    "    done, episode_return = False, 0.\n",
    "    eval = False\n",
    "\n",
    "    for global_step in range(1, total_max_steps+1):\n",
    "        a = agent.act(s)\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        episode_return += r\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # train step\n",
    "        agent.append_to_replay_buffer(s, a, r, s_next, terminated)\n",
    "        agent.update(train_schedule)\n",
    "\n",
    "        # evaluate\n",
    "        if global_step % eval_schedule == 0:\n",
    "            eval = True\n",
    "\n",
    "        s = s_next\n",
    "        if done:\n",
    "            if eval:\n",
    "                episode_return_history.append(episode_return)\n",
    "                avg_return = np.mean(episode_return_history)\n",
    "                print(f'{global_step=} | {avg_return=:.3f}')\n",
    "                if avg_return >= success_ret:\n",
    "                    print('Решено!')\n",
    "                    break\n",
    "\n",
    "            s, _ = env.reset()\n",
    "            done, episode_return = False, 0.\n",
    "            eval = False\n",
    "\n",
    "run_actor_critic(eval_schedule=2000, total_max_steps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86a65e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=2008 | avg_return=9.000\n",
      "global_step=4008 | avg_return=9.500\n",
      "global_step=6002 | avg_return=9.667\n",
      "global_step=8014 | avg_return=23.250\n",
      "global_step=10184 | avg_return=61.600\n",
      "global_step=12051 | avg_return=85.333\n",
      "global_step=14004 | avg_return=100.000\n",
      "global_step=16056 | avg_return=108.875\n",
      "global_step=18127 | avg_return=119.111\n",
      "global_step=20073 | avg_return=125.000\n",
      "global_step=22112 | avg_return=147.600\n",
      "global_step=24124 | avg_return=164.700\n",
      "global_step=26312 | avg_return=195.700\n",
      "global_step=28099 | avg_return=211.900\n",
      "global_step=30185 | avg_return=224.600\n",
      "global_step=32206 | avg_return=227.700\n",
      "global_step=34010 | avg_return=229.000\n",
      "global_step=36064 | avg_return=234.100\n",
      "global_step=38076 | avg_return=239.700\n",
      "global_step=40319 | avg_return=254.000\n",
      "global_step=42240 | avg_return=264.900\n",
      "global_step=44126 | avg_return=267.700\n",
      "global_step=46240 | avg_return=270.800\n",
      "global_step=48042 | avg_return=281.300\n",
      "global_step=50116 | avg_return=272.100\n",
      "global_step=52038 | avg_return=291.300\n",
      "global_step=54212 | avg_return=321.200\n",
      "global_step=56047 | avg_return=324.000\n",
      "global_step=58484 | avg_return=348.300\n",
      "global_step=60074 | avg_return=336.600\n",
      "global_step=62041 | avg_return=322.300\n",
      "global_step=64069 | avg_return=322.300\n",
      "global_step=66060 | avg_return=314.700\n",
      "global_step=68116 | avg_return=307.400\n",
      "global_step=70007 | avg_return=308.700\n",
      "global_step=72259 | avg_return=295.500\n",
      "global_step=74325 | avg_return=281.000\n",
      "global_step=76193 | avg_return=288.600\n",
      "global_step=78186 | avg_return=288.600\n",
      "global_step=80400 | avg_return=316.400\n",
      "global_step=82074 | avg_return=341.200\n",
      "global_step=84312 | avg_return=370.300\n",
      "global_step=86312 | avg_return=392.800\n",
      "global_step=88210 | avg_return=403.100\n",
      "global_step=90043 | avg_return=424.800\n",
      "global_step=92157 | avg_return=445.300\n",
      "global_step=94300 | avg_return=459.800\n",
      "global_step=96300 | avg_return=477.200\n",
      "global_step=98300 | avg_return=477.200\n",
      "global_step=100300 | avg_return=479.000\n",
      "global_step=102300 | avg_return=484.100\n",
      "global_step=104300 | avg_return=484.100\n",
      "global_step=106300 | avg_return=484.100\n",
      "global_step=108300 | avg_return=498.000\n",
      "global_step=110300 | avg_return=500.000\n",
      "Решено!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Коммент:\n",
    "1. Архитектура нейронной сети:\n",
    "- Первая версия использует общую сеть (shared network) для \n",
    "  актора и критика с общим телом (self.net) и двумя головами\n",
    "\n",
    "- Вторая версия использует полностью раздельные сети для актора \n",
    "  и критика (self.net = None), что обычно считается более надежным подходом\n",
    "\n",
    "2. Оптимизаторы\n",
    "- Первая версия использует один оптимизатор для всей сети\n",
    "- Вторая версия использует отдельные оптимизаторы для актора и критика с разными скоростями обучения:\n",
    "\n",
    "3. Процесс обновления:\n",
    "- Вторая версия делает несколько обновлений критика на каждое \n",
    "обновление актора (critic_updates_per_actor=4)\n",
    "\n",
    "\"\"\"\n",
    "class ActorBatch:\n",
    "    def __init__(self):\n",
    "        self.logprobs = []\n",
    "        self.q_values = []\n",
    "        \n",
    "    def append(self, log_prob, q_value):\n",
    "        self.logprobs.append(log_prob)\n",
    "        self.q_values.append(q_value)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.logprobs.clear()\n",
    "        self.q_values.clear()\n",
    "\n",
    "    \n",
    "class ActorCriticModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Инициализируйте сеть агента с двумя головами: softmax-актора и линейного критика\n",
    "        # self.net, self.actor_head, self.critic_head =\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        def create_body():\n",
    "            from_dim = input_dim\n",
    "            layers = []\n",
    "            for to_dim in hidden_dims:\n",
    "                layers.extend([\n",
    "                    nn.Linear(from_dim, to_dim),\n",
    "                    nn.Tanh()\n",
    "                ])\n",
    "                from_dim = to_dim\n",
    "            return layers\n",
    "        \n",
    "        # self.net = nn.Sequential(*create_body())\n",
    "        # self.actor_head = nn.Sequential(\n",
    "        #     nn.Linear(hidden_dims[-1], output_dim),\n",
    "        #     SymExpModule(),\n",
    "        #     nn.Softmax(dim=-1),\n",
    "        # )\n",
    "        # self.critic_head = nn.Sequential(\n",
    "        #     nn.Linear(hidden_dims[-1], output_dim),\n",
    "        #     SymExpModule(),\n",
    "        # )\n",
    "\n",
    "        # NB: Или можно было бы сделать раздельные сетки под актора и критика\n",
    "        # без общего тела, что обычно и рекомендуется:\n",
    "        self.net = None # Нет общего тела\n",
    "\n",
    "        # Сеть актора (policy network)\n",
    "        self.actor_head = nn.Sequential(\n",
    "            *create_body(),\n",
    "            nn.Linear(hidden_dims[-1], output_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        # Сеть критика (value network)\n",
    "        self.critic_head = nn.Sequential(\n",
    "            *create_body(),\n",
    "            nn.Linear(hidden_dims[-1], output_dim),\n",
    "        )\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Вычислите выбранное действие, логарифм вероятности его выбора и соответствующее значение Q-функции\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        if self.net is not None:\n",
    "            state = self.net(state)\n",
    "        \n",
    "        # Получаем вероятности действий от актора\n",
    "        action_probs = self.actor_head(state)\n",
    "\n",
    "        # Создаем категориальное распределение\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        # Выбираем случайное действие согласно распределению\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Вычисляем логарифм вероятности выбранного действия\n",
    "        log_prob = dist.log_prob(action)\n",
    "        action = action.item()\n",
    "        \n",
    "        # Вычисляем Q-значения без градиентов\n",
    "        with torch.no_grad():\n",
    "            Q_s = self.critic_head(state)\n",
    "            Q_s_a = Q_s[action] # Q-значение для выбранного действия\n",
    "\n",
    "            # NB: Или можно было бы возвращать для градиента стратегии не Q, а A — преимущество\n",
    "            V_s, _ = torch.max(Q_s, axis=-1) # Максимальное Q-значение как оценка V(s)\n",
    "            advantage = Q_s_a - V_s # Насколько выбранное действие лучше среднего\n",
    "            Q_s_a = advantage # Используем преимущество вместо Q-значения\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "        return action, log_prob, Q_s_a\n",
    "    \n",
    "    def evaluate(self, state):\n",
    "        # Вычислите значения Q-функции для данного состояния\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        if self.net is not None:\n",
    "            state = self.net(state)\n",
    "        \n",
    "        q_values = self.critic_head(state)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims, lr, gamma, critic_rb_size):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Инициализируйте модель актор-критика и SGD оптимизатор (например, `torch.optim.Adam)`)\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        self.actor_critic = ActorCriticModel(state_dim, hidden_dims, action_dim)\n",
    "        \n",
    "        # self.actor_opt = self.critic_opt = self.opt = torch.optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        # NB: Можно использовать один оптимизатор на всех ^^^^, но по-хорошему имеет смысл\n",
    "        # сделать каждому свой со своей скоростью обучения.\n",
    "        self.actor_opt = torch.optim.Adam(self.actor_critic.actor_head.parameters(), lr=lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.actor_critic.critic_head.parameters(), lr=lr/2)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "\n",
    "        self.actor_batch = ActorBatch()\n",
    "        self.critic_rb = deque(maxlen=critic_rb_size)\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Произведите выбор действия и сохраните необходимые данные в батч для последующего обучения\n",
    "        # Не забудьте сделать q_value.detach()\n",
    "        # self.actor_batch.append(..)\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        action, logprob, q_value = self.actor_critic(to_tensor(state))\n",
    "        self.actor_batch.append(logprob, q_value.detach())\n",
    "\n",
    "        # NB: Для дебага, можно сначала обучать только критика и убедиться, что DQN работает\n",
    "        # action = select_action_eps_greedy(self.actor_critic.critic_head, state, 0.05)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def append_to_replay_buffer(self, s, a, r, next_s, terminated):\n",
    "        # Добавьте новый экземпляр данных в память прецедентов.\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        self.critic_rb.append((s, a, r, next_s, terminated))\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "    def evaluate(self, state):\n",
    "        return self.actor_critic.evaluate(state)\n",
    "    \n",
    "    def update(self, rollout_size, critic_batch_size, critic_updates_per_actor):\n",
    "        if len(self.actor_batch.q_values) < rollout_size:\n",
    "            return\n",
    "\n",
    "        # Сначала обновляем критика несколько раз\n",
    "        self.update_critic(critic_batch_size, critic_updates_per_actor)\n",
    "\n",
    "        # Затем обновляем актора\n",
    "        self.update_actor()\n",
    "\n",
    "    def update_actor(self):\n",
    "        # Получаем сохраненные данные\n",
    "        Q_s_a = to_tensor(self.actor_batch.q_values)\n",
    "        logprobs = torch.stack(self.actor_batch.logprobs)\n",
    "\n",
    "        # Реализуйте шаг обновления актора — вычислите ошибку `loss` и произведите шаг обновления градиентным спуском. \n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        # Считаем ошибку\n",
    "        loss = -torch.mean(Q_s_a * logprobs)\n",
    "        \n",
    "        # Обновляем веса актора\n",
    "        self.actor_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_opt.step()\n",
    "        self.actor_batch.clear()\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "    def update_critic(self, batch_size, n_updates=1):\n",
    "        # Реализуйте n_updates шагов обучения критика.\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "\n",
    "        if len(self.critic_rb) < batch_size:\n",
    "            return\n",
    "\n",
    "        # Ограничиваем число обновлений доступными данными\n",
    "        if len(self.critic_rb) < batch_size * n_updates:\n",
    "            n_updates = len(self.critic_rb) // batch_size\n",
    "\n",
    "        # Делаем несколько обновлений\n",
    "        for _ in range(n_updates):\n",
    "            train_batch = sample_batch(self.critic_rb, batch_size)\n",
    "            states, actions, rewards, next_states, terminateds = train_batch\n",
    "            \n",
    "            # Обновляем веса критика\n",
    "            self.critic_opt.zero_grad()\n",
    "            loss = self.compute_td_loss(states, actions, rewards, next_states, terminateds)\n",
    "            loss.backward()\n",
    "            self.critic_opt.step()\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "        \n",
    "    def compute_td_loss(\n",
    "        self, states, actions, rewards, next_states, terminateds, regularizer=0.1\n",
    "    ):\n",
    "        # переводим входные данные в тензоры\n",
    "        s = to_tensor(states)                     # shape: [batch_size, state_size]\n",
    "        a = to_tensor(actions, int).long()        # shape: [batch_size]\n",
    "        r = to_tensor(rewards)                    # shape: [batch_size]\n",
    "        s_next = to_tensor(next_states)           # shape: [batch_size, state_size]\n",
    "        term = to_tensor(terminateds, bool)       # shape: [batch_size]\n",
    "\n",
    "        \n",
    "        # получаем Q[s, a] для выбранных действий в текущих состояниях (для каждого примера из батча)\n",
    "        # Q_s_a = ...\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        Q_s_a = torch.gather(\n",
    "            self.evaluate(s), dim=1, index=torch.unsqueeze(a, 1)\n",
    "        ).squeeze(1)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "        # получаем Q[s_next, *] — значения полезности всех действий в следующих состояниях\n",
    "        # Q_sn = ...,\n",
    "        # а затем вычисляем V*[s_next] — оптимальные значения полезности следующих состояний\n",
    "        # V_sn = ...\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        with torch.no_grad():\n",
    "            Q_sn = self.evaluate(s_next)\n",
    "            V_sn, _ = torch.max(Q_sn, axis=-1)\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "        # вычисляем TD target и далее TD error\n",
    "        # target = ...\n",
    "        # td_error = ...\n",
    "        \"\"\"<codehere>\"\"\"\n",
    "        target = r + self.gamma * V_sn * torch.logical_not(term)\n",
    "        td_error = target - Q_s_a\n",
    "        \"\"\"</codehere>\"\"\"\n",
    "    \n",
    "        # MSE loss для минимизации\n",
    "        loss = torch.mean(td_error ** 2)\n",
    "        # добавляем регуляризацию на значения Q \n",
    "        loss += regularizer * Q_s_a.mean()\n",
    "        return loss\n",
    "\n",
    "def run_actor_critic(\n",
    "        env_name=\"CartPole-v1\", \n",
    "        hidden_dims=(128, 128), lr=5e-4,\n",
    "        total_max_steps=500_000,\n",
    "        train_schedule=16, replay_buffer_size=50000, batch_size=64, critic_updates_per_actor=4,\n",
    "        eval_schedule=1000, smooth_ret_window=10, success_ret=500.\n",
    "):\n",
    "    env = gym.make(env_name)\n",
    "    episode_return_history = deque(maxlen=smooth_ret_window)\n",
    "\n",
    "    agent = ActorCriticAgent(\n",
    "        state_dim=env.observation_space.shape[0], action_dim=env.action_space.n, hidden_dims=hidden_dims,\n",
    "        lr=lr, gamma=.995, critic_rb_size=replay_buffer_size\n",
    "    )\n",
    "    \n",
    "    s, _ = env.reset()\n",
    "    done, episode_return = False, 0.\n",
    "    eval = False\n",
    "\n",
    "    for global_step in range(1, total_max_steps+1):\n",
    "        a = agent.act(s)\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        episode_return += r\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # train step\n",
    "        agent.append_to_replay_buffer(s, a, r, s_next, terminated)\n",
    "        agent.update(train_schedule, batch_size, critic_updates_per_actor)\n",
    "\n",
    "        # evaluate\n",
    "        if global_step % eval_schedule == 0:\n",
    "            eval = True\n",
    "\n",
    "        s = s_next\n",
    "        if done:\n",
    "            if eval:\n",
    "                episode_return_history.append(episode_return)\n",
    "                avg_return = np.mean(episode_return_history)\n",
    "                print(f'{global_step=} | {avg_return=:.3f}')\n",
    "                if avg_return >= success_ret:\n",
    "                    print('Решено!')\n",
    "                    break\n",
    "\n",
    "            s, _ = env.reset()\n",
    "            done, episode_return = False, 0.\n",
    "            eval = False\n",
    "\n",
    "run_actor_critic(eval_schedule=2000, total_max_steps=200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0ef4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
