{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch \n",
    "# !pip install transformers \n",
    "# !pip install datasets \n",
    "# !pip install trl \n",
    "# !pip install peft \n",
    "# !pip install accelerate \n",
    "# !pip install bitsandbytes \n",
    "# !pip install scikit-learn \n",
    "# !pip install numpy \n",
    "# !pip install wandb\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "import trl\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "print(f\"trl version: {trl.__version__}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Selection and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for small models\n",
    "model_options = {\n",
    "    \"tinyllama\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
    "    \"gpt2\": \"gpt2\",  # 124M parameters\n",
    "    \"gpt2-medium\": \"gpt2-medium\",  # 355M parameters\n",
    "    \"opt-125m\": \"facebook/opt-125m\",\n",
    "    \"bloom-560m\": \"bigscience/bloom-560m\"\n",
    "}\n",
    "\n",
    "# Choose a model\n",
    "MODEL_CHOICE = \"tinyllama\"\n",
    "model_name = model_options[MODEL_CHOICE]\n",
    "\n",
    "print(f\"Selected model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with optimal settings for limited resources\n",
    "device_map = \"auto\"\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"CUDA unavailable, using MPS (Metal Performance Shaders) for Mac\")\n",
    "    device_map = {\"\": \"mps\"}\n",
    "else:\n",
    "    print(\"CUDA and MPS unavailable, using CPU\")\n",
    "    device_map = {\"\": \"cpu\"}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_math_problem():\n",
    "    \"\"\"Generates a random math problem.\"\"\"\n",
    "    operations = [\"+\", \"-\", \"*\", \"/\"]\n",
    "    op = random.choice(operations)\n",
    "    \n",
    "    if op == \"+\":\n",
    "        a = random.randint(1, 100)\n",
    "        b = random.randint(1, 100)\n",
    "        answer = a + b\n",
    "        prompt = f\"What is {a} plus {b}?\"\n",
    "    elif op == \"-\":\n",
    "        a = random.randint(1, 100)\n",
    "        b = random.randint(1, min(a, 100))  # To ensure a positive result\n",
    "        answer = a - b\n",
    "        prompt = f\"What is {a} minus {b}?\"\n",
    "    elif op == \"*\":\n",
    "        a = random.randint(1, 20)\n",
    "        b = random.randint(1, 20)\n",
    "        answer = a * b\n",
    "        prompt = f\"What is {a} multiplied by {b}?\"\n",
    "    else:  # \"/\"\n",
    "        b = random.randint(1, 10)\n",
    "        a = b * random.randint(1, 10)  # To ensure an integer result\n",
    "        answer = a // b\n",
    "        prompt = f\"What is {a} divided by {b}?\"\n",
    "    \n",
    "    return prompt, answer\n",
    "\n",
    "# Create training and validation datasets\n",
    "def create_datasets(train_size=200, val_size=50):\n",
    "    \"\"\"Creates training and validation datasets.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Generate data\n",
    "    for _ in range(train_size + val_size):\n",
    "        prompt, answer = generate_math_problem()\n",
    "        all_data.append({\"prompt\": prompt, \"answer\": answer})\n",
    "    \n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    train_data = all_data[:train_size]\n",
    "    val_data = all_data[train_size:]\n",
    "    \n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = create_datasets(train_size=1000, val_size=50)\n",
    "\n",
    "print(\"Examples from the training dataset:\")\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}: {train_dataset[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Base Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(problem):\n",
    "    \"\"\"Formats a problem as a prompt for the model.\"\"\"\n",
    "    return f\"Problem: {problem}\\nAnswer:\"\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    min_new_tokens=1,\n",
    "    max_new_tokens=4,\n",
    "    # device=\"cpu\"  # Force CPU for the pipeline\n",
    ")\n",
    "\n",
    "# Test the base model on a few examples\n",
    "print(\"\\nTesting the base model:\")\n",
    "for i in range(3):\n",
    "    prompt = train_dataset[i][\"prompt\"]\n",
    "    expected = train_dataset[i][\"answer\"]\n",
    "    prompt = format_prompt(prompt)\n",
    "    response = pipe(prompt)[0][\"generated_text\"]\n",
    "    \n",
    "    print(f\"\\nProblem: {prompt}\")\n",
    "    print(f\"Expected answer: {expected}\")\n",
    "    print(f\"Model response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function for evaluating model responses.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompts: list of problems\n",
    "    - completions: list of model responses\n",
    "    - kwargs: additional arguments that GRPOTrainer may pass\n",
    "    \n",
    "    Returns:\n",
    "    - list of rewards for each response\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    ground_truth = kwargs.get('answer', [None] * len(prompts))\n",
    "    \n",
    "    for prompt, completion, gt in zip(prompts, completions, ground_truth):\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Extract numerical answer\n",
    "        match = re.search(r'Answer:\\s*(\\d+)', completion)\n",
    "        \n",
    "        format_correct = bool(match)\n",
    "        \n",
    "        if format_correct:\n",
    "            reward += 1.0\n",
    "            \n",
    "            # Check answer correctness if ground_truth is available\n",
    "            if gt is not None:\n",
    "                try:\n",
    "                    provided_answer = int(match.group(1))\n",
    "                    \n",
    "                    # Differentiated reward based on closeness to correct answer\n",
    "                    if provided_answer == gt:\n",
    "                        reward += 2.0  # Full reward for correct answer\n",
    "                    elif abs(provided_answer - gt) <= 5:\n",
    "                        reward += 1.0  # Partial reward for close answer\n",
    "                    elif abs(provided_answer - gt) <= 10:\n",
    "                        reward += 0.5  # Small reward for not too distant answer\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. GRPO Trainer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO configuration with optimal parameters for small models\n",
    "grpo_config = GRPOConfig(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # beta=0.1,  # Target KL divergence\n",
    "    seed=SEED,\n",
    "    scale_rewards=True,  # Reward scaling\n",
    "    output_dir=\"./grpo_checkpoint\",  # Directory for checkpoint saving\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
    "    eval_strategy=\"epoch\",  # Evaluate model at the end of each epoch\n",
    "    num_train_epochs=3,\n",
    "    max_completion_length=4,\n",
    "    report_to=[\"wandb\"],  # Enable wandb reports\n",
    "    log_completions=True,  # Enable text example logging\n",
    "    wandb_log_unique_prompts=True  # Log only unique prompts to save space\n",
    ")\n",
    "\n",
    "# GRPO trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=grpo_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    reward_funcs=[reward_function],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_grpo(dataset):\n",
    "    \"\"\"Prepares data for GRPO training.\"\"\"\n",
    "    problems = [format_prompt(item['prompt']) for item in dataset]\n",
    "    expected_answers = [item['answer'] for item in dataset]\n",
    "    \n",
    "    initial_responses = []\n",
    "    print(\"Getting initial model responses...\")\n",
    "    \n",
    "    cpu_model = model.to(\"cpu\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for problem in tqdm(problems[:10]):\n",
    "            inputs = tokenizer(problem, return_tensors=\"pt\")\n",
    "            outputs = cpu_model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens=4,\n",
    "                min_new_tokens=1,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            initial_responses.append(response)\n",
    "    \n",
    "    cpu_model = cpu_model.to(model.device)\n",
    "    \n",
    "    initial_rewards = reward_function(problems[:10], initial_responses, answer=expected_answers[:10])\n",
    "    avg_initial_reward = sum(initial_rewards) / len(initial_rewards)\n",
    "    \n",
    "    print(f\"Average initial reward on sample: {avg_initial_reward:.2f}\")\n",
    "\n",
    "\n",
    "def evaluate_model_for_wandb(model, tokenizer, test_problems=None, epoch=0):\n",
    "    \"\"\"\n",
    "    Evaluates the model on test examples and logs results to wandb.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained model\n",
    "    - tokenizer: tokenizer\n",
    "    - test_problems: list of test problems (if None, predefined ones are used)\n",
    "    - epoch: epoch number\n",
    "    \"\"\"\n",
    "    if test_problems is None:\n",
    "        test_problems = [\n",
    "            \"What is 15 plus 27?\",\n",
    "            \"What is 42 minus 18?\",\n",
    "            \"What is 7 multiplied by 8?\", \n",
    "            \"What is 45 divided by 9?\"\n",
    "        ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for problem in test_problems:\n",
    "        formatted_prompt = format_prompt(problem)\n",
    "        \n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=4,\n",
    "            min_new_tokens=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        match = re.search(r'Answer:\\s*(\\d+)', response)\n",
    "        answer = match.group(1) if match else \"Invalid format\"\n",
    "    \n",
    "        results.append({\n",
    "            \"Problem\": problem,\n",
    "            \"Model Response\": response,\n",
    "            \"Numerical Answer\": answer\n",
    "        })\n",
    "    \n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"model_samples/epoch\": epoch,\n",
    "            \"model_samples/examples\": wandb.Table(\n",
    "                dataframe=pd.DataFrame(results)\n",
    "            )\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "class WandbEvalCallback(TrainerCallback):\n",
    "    \"\"\"Callback for logging generation results to wandb after each epoch\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, test_problems=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_problems = test_problems\n",
    "        \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        evaluate_model_for_wandb(\n",
    "            self.model, \n",
    "            self.tokenizer, \n",
    "            self.test_problems, \n",
    "            state.epoch\n",
    "        )\n",
    "        return control\n",
    "\n",
    "if \"wandb\" in grpo_config.report_to:\n",
    "    test_problems = [\n",
    "        \"What is 15 plus 27?\",\n",
    "        \"What is 42 minus 18?\",\n",
    "        \"What is 7 multiplied by 8?\",\n",
    "        \"What is 45 divided by 9?\",\n",
    "        \"What is 33 plus 44?\",\n",
    "        \"What is 99 minus 34?\",\n",
    "        \"What is 12 multiplied by 5?\",\n",
    "        \"What is 72 divided by 8?\"\n",
    "    ]\n",
    "    trainer.add_callback(WandbEvalCallback(model, tokenizer, test_problems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_dataset.select(range(50))\n",
    "prepare_dataset_for_grpo(train_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting GRPO training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Final Comparison Before/After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal comparison of model before and after training:\")\n",
    "\n",
    "# Load original model (before training))\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "original_pipe = pipe\n",
    "\n",
    "test_problems = [\n",
    "    \"What is 15 plus 27?\",\n",
    "    \"What is 42 minus 18?\",\n",
    "    \"What is 7 multiplied by 8?\",\n",
    "    \"What is 45 divided by 9?\"\n",
    "]\n",
    "\n",
    "print(\"\\nComparison of responses on new examples:\")\n",
    "for problem in test_problems:\n",
    "    # Original model\n",
    "    original_prompt = format_prompt(problem)\n",
    "    original_response = original_pipe(original_prompt)[0][\"generated_text\"]\n",
    "    \n",
    "    # Trained model\n",
    "    trained_prompt = format_prompt(problem)\n",
    "    inputs = tokenizer(trained_prompt, return_tensors=\"pt\").to(trainer.model.device)\n",
    "    outputs = trainer.model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=4,\n",
    "        min_new_tokens=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    trained_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nProblem: {problem}\")\n",
    "    print(f\"Original model: {original_response}\")\n",
    "    print(f\"Trained model: {trained_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
